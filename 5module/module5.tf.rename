#Scenario: add S3 bucket to load HTML files to web servers
#get logs from instances and add them to the S3 bucket

#VARIABLES

variable "aws_access_key" {}
variable "aws_secret_key" {}
variable "private_key_path" {}
variable "key_name" {}
variable "region" {
  default = "us-west-2"
}
variable "network_address_space" {
    default = "10.1.0.0/16"
}
variable "subnet1_address_space" {
    default = "10.1.0.0/24"
}
variable "subnet2_address_space" {
  default = "10.1.1.0/24"
}

variable "bucket_name_prefix" {}
variable "billing_code_tag" {}
variable "environment_tag" {}

#PROVIDERS

 provider "aws" {
  access_key = var.aws_access_key
  secret_key = var.aws_secret_key
  region = var.region
 }
 
#LOCALS

locals {
    common_tags = {
        BillingCode = var.billing_code_tag
        Environment = var.environment_tag
    }
    s3_bucket_name = "${var.bucket_name_prefix}-${var.environment_tag}-${random_integer.rand.result}"
}




 #DATA
 #data is us retreiving info from AWS
 #in this case we want an AMI id and the available AZ's
 data "aws_availability_zones" "available" {}

 data "aws_ami" "aws-linux" {
  most_recent = true
  owners = ["amazon"]

  filter {
    name = "name"
    values = ["amzn-ami-hvm*"]
  }

  filter {
    name = "root-device-type"
    values = ["ebs"]
  }

  filter {
    name = "virtualization-type"
    values = ["hvm"]
  }
}

#RESOURCES

#RANDOM ID

#use this random integer resource to add to the end of our bucket name
#we do this b/c buckets have to be globally unique and this is an easy
#almost sure way of making sure the bucket will have a unique name
resource "random_integer" "rand" {
    min = 10000
    max = 99999
}


#NETWORKING
#for vpc resource we add tags value
#we are using the merge function to apply 2 tags
#the common tag and the environment tag we set as variables above in
#locals 
#the merge function below combines 2 map values into one map
#using the local.common_tags map
#and then making a map on the fly with the rest of the stuff after the ,

resource "aws_vpc" "vpc" {
    cidr_block = var.network_address_space
    
    tags = merge(local.common_tags, { Name = "${var.environment_tag}-vpc"})
}

resource "aws_internet_gateway" "igw" {
    vpc_id = aws_vpc.vpc.id

    tags = merge(local.common_tags, { Name = "${var.environment_tag}-igw"})
}

resource "aws_subnet" "subnet1" {
    cidr_block = var.subnet1_address_space 
    vpc_id = aws_vpc.vpc.id 
    #map public ip on launch is set to true so when instances are
    #spinned up on this subnet they get a public and private IP
    map_public_ip_on_launch = "true"
    availability_zone = data.aws_availability_zones.available.names[0]

}

resource "aws_subnet" "subnet2" {
  cidr_block = var.subnet2_address_space
  vpc_id = aws_vpc.vpc.id 
  map_public_ip_on_launch = "true"
  availability_zone = data.aws_availability_zones.available.names[1]
}

#ROUTING

resource "aws_route_table" "rtb" {
    vpc_id = aws_vpc.vpc.id 

    route {
        cidr_block = "0.0.0.0/0"
        gateway_id = aws_internet_gateway.igw.id 
    }
}

resource "aws_route_table_association" "rta-subnet1" {
    subnet_id = aws_subnet.subnet1.id 
    route_table_id = aws_route_table.rtb.id 
}

resource "aws_route_table_association" "rta-subnet2" {
  subnet_id = aws_subnet.subnet2.id 
  route_table_id = aws_route_table.rtb.id
}

#SECURITY GROUPS

resource "aws_security_group" "elb-sg" {
  name = "nginx_elb_sg"
  vpc_id = aws_vpc.vpc.id 

    #allow HTTP from anywhere
  ingress {
    from_port = 80
    to_port = 80
    protocol = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  #allow all outbound
  egress {
    from_port = 0
    to_port = 0
    protocol = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }
}

resource "aws_security_group" "nginx-sg" {
  name = "nginx_sg"
  description = "Allow ports for nginx demo"
  vpc_id = aws_vpc.vpc.id 

  #SSH from anywhere
  ingress {
    from_port = 22
    to_port = 22
    protocol = "tcp"
    cidr_blocks =["0.0.0.0/0"]
  }
  #HTTP access from VPC
  ingress {
    from_port = 80
    to_port = 80
    protocol = "tcp"
    cidr_blocks = [var.network_address_space]
  }
  #Outbound internet access to anywhere
  egress {
    from_port = 0
    to_port = 0
    protocol = -1
    cidr_blocks = ["0.0.0.0/0"]
  }
}

#LOAD BALANCER
resource "aws_elb" "web" {
  name = "nginx-elb"

  subnets = [aws_subnet.subnet1.id, aws_subnet.subnet2.id]
  security_groups = [aws_security_group.elb-sg.id]
  instances = [aws_instance.nginx1.id, aws_instance.nginx2.id]

  listener {
    instance_port = 80
    instance_protocol = "http"
    lb_port = 80
    lb_protocol = "http"
  }
}

#INSTANCES
#below we have instance profile that gives the permissions necessary to this
#instance to access the S3 bucket we are creating to serve up the HTML
#page and send back logs
resource "aws_instance" "nginx1" {
    ami = data.aws_ami.aws-linux.id 
    instance_type = "t2.micro"
    subnet_id = aws_subnet.subnet1.id 
    vpc_security_group_ids = [aws_security_group.nginx-sg.id]
    key_name = var.key_name 
    iam_instance_profile = aws_iam_instance_profile.nginx_profile.name

    connection {
        type = "ssh"
        host = self.public_ip
        user = "ec2-user"
        private_key = file(var.private_key_path)

    }

#defining the file we wish to make in the instance using content = <<EOF
#this is a special way to indicate that anything that follows the <<EOF
#should be copied exactly as is 
#end it with another EOF
#add the destination
    provisioner "file" {
        content = <<EOF
access_key =
secret_key =
security_token =
use_https = True
bucket_location = US

EOF
        destination = "/home/ec2-user/.s3cfg"
    }
#in below file provisioner we are using logrotate to rotate the logs into the S3 bucket
#allows us to run a script after the log rotate happens
#after last action we get the instance ID via curl and putting it in that variable
#using the s3cmd with sync on, adding the configuration, source directory from
#where it should copy files and lastly the destination
    provisioner "file" {
        content = <<EOF
/var/log/nginx/*log {
    daily
    rotate 10
    missingok
    compress
    sharedscripts
    postrotate
    endscript
    lastaction
        INSTANCE_ID = 'curl --silent http://169.254.169.254/latest/meta-data/instance-id'
        sudo /usr/local/bin/s3cmd -sync --config=/home/ec2-user/.s3cfg /var/log/nginx/ s3://{aws_s3_bucket.web_bucket.id}/nginx/$INSTANCE_ID/
    endscript
}
EOF
    destination = "/home/ec2-user/nginx"
    }
    
    
    
    provisioner "remote-exec" {
        inline = [
            "sudo yum install nginx -y",
            "sudo service nginx start",
            "sudo cp /home/ec2-user/.s3cfg /root/.s3cfg",
            "sudo cp /home/ec2-user/nginx /etc/logrotate.d/nginx",
            "sudo pip install s3cmd",
            "s3cmd get s3://${aws_s3_bucket.web_bucket.id}/website/index.html .",
            "s3cmd get s3://${aws_s3_bucket.web_bucket.id}/website/Globo_logo_Vert.png .",
            "sudo rm /usr/share/nginx/html/index.html",
            "sudo cp /home/ec2-user/index.html /usr/share/nginx/html/index.html",
            "sudo cp /home/ec2-user/Globo_logo_Vert.png /usr/share/nginx/html/Globo_logo_Vert.png",
            "sudo logrotate -f /etc/logrotate.conf"
        ]
    }

    tags = merge(local.common_tags, { Name = "${var.environment_tag}-nginx1" })

}

#2ND instance

resource "aws_instance" "nginx2" {
  ami                    = data.aws_ami.aws-linux.id
  instance_type          = "t2.micro"
  subnet_id              = aws_subnet.subnet2.id
  vpc_security_group_ids = [aws_security_group.nginx-sg.id]
  key_name               = var.key_name
  iam_instance_profile   = aws_iam_instance_profile.nginx_profile.name

  connection {
    type        = "ssh"
    host        = self.public_ip
    user        = "ec2-user"
    private_key = file(var.private_key_path)

  }

  provisioner "file" {
    content = <<EOF
access_key =
secret_key =
security_token =
use_https = True
bucket_location = US
EOF
    destination = "/home/ec2-user/.s3cfg"
  }

  provisioner "file" {
    content = <<EOF
/var/log/nginx/*log {
    daily
    rotate 10
    missingok
    compress
    sharedscripts
    postrotate
    endscript
    lastaction
        INSTANCE_ID=`curl --silent http://169.254.169.254/latest/meta-data/instance-id`
        sudo /usr/local/bin/s3cmd sync --config=/home/ec2-user/.s3cfg /var/log/nginx/ s3://${aws_s3_bucket.web_bucket.id}/nginx/$INSTANCE_ID/
    endscript
}
EOF
    destination = "/home/ec2-user/nginx"
  }

  provisioner "remote-exec" {
    inline = [
      "sudo yum install nginx -y",
      "sudo service nginx start",
      "sudo cp /home/ec2-user/.s3cfg /root/.s3cfg",
      "sudo cp /home/ec2-user/nginx /etc/logrotate.d/nginx",
      "sudo pip install s3cmd",
      "s3cmd get s3://${aws_s3_bucket.web_bucket.id}/website/index.html .",
      "s3cmd get s3://${aws_s3_bucket.web_bucket.id}/website/Globo_logo_Vert.png .",
      "sudo rm /usr/share/nginx/html/index.html",
      "sudo cp /home/ec2-user/index.html /usr/share/nginx/html/index.html",
      "sudo cp /home/ec2-user/Globo_logo_Vert.png /usr/share/nginx/html/Globo_logo_Vert.png",
      "sudo logrotate -f /etc/logrotate.conf"
      
    ]
  }

  tags = merge(local.common_tags, { Name = "${var.environment_tag}-nginx2" })

}








#S3 bucket config
#json straight from AWS is the role for S3 allowance
resource "aws_iam_role" "allow_nginx_s3" {
  name = "allow_nginx_s3"

  assume_role_policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Effect": "Allow",
      "Sid": ""
    }
  ]
}
EOF
}

#define instance profile that we are handing each instance and the 
#role they are getting
resource "aws_iam_instance_profile" "nginx_profile" {
    name = "nginx_profile"
    role = aws_iam_role.allow_nginx_s3.name 
}

#the policy for the role created above
#policy below grants all permission for s3
resource "aws_iam_role_policy" "allow_s3_all" {
  name = "allow_s3_all"
  role = aws_iam_role.allow_nginx_s3.name

  policy = <<EOF
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "s3:*"
      ],
      "Effect": "Allow",
      "Resource": [
                "arn:aws:s3:::${local.s3_bucket_name}",
                "arn:aws:s3:::${local.s3_bucket_name}/*"
            ]
    }
  ]
}
EOF

  }

#creating bucket
resource "aws_s3_bucket" "web_bucket" {
    bucket = local.s3_bucket_name
    acl = "private"
    force_destroy = true

    tags = merge(local.common_tags, { Name = "${var.environment_tag}-web-bucket" })
}

#uploading html file and png file for next 2 resources
resource "aws_s3_bucket_object" "website" {
    bucket = aws_s3_bucket.web_bucket.bucket 
    key = "/website/index.html"
    source = "./index.html"
}

resource "aws_s3_bucket_object" "graphic" {
    bucket = aws_s3_bucket.web_bucket.bucket 
    key = "/website/Globo_logo_Vert.png"
    source = "./Globo_logo_Vert.png"

}


#OUTPUT

output "aws_elb_public_dns" {
    value = aws_elb.web.dns_name
}